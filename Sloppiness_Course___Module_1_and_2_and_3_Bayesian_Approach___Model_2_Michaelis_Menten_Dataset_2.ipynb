{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjNEGwk3RPO0"
   },
   "source": [
    "# 1. Model-data calibration\n",
    "In this module, we are going to be focusing on coding up model-data calibration using Bayesian and frequentist approaches.\n",
    "\n",
    "In this particular Python script, we are working on the **Bayesian** approach, for 3 model examples:\n",
    "1.    A toy example (fitting an exponential model to some toy data).\n",
    "2.    Michaelis-Menten kinetics for chemical reactions (as in [Monsalve-Bravo *et al.* 2022 Sci. Adv.](https://www.science.org/doi/10.1126/sciadv.abm5952)).\n",
    "3.    Logistic growth model for coral reef recovery (as in [Simpson *et al.* 2022 J. Theor. Biol.](https://www.sciencedirect.com/science/article/pii/S0022519321004185))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Yvdq_FlRVii"
   },
   "source": [
    "### 1.1 Preliminary code to run!\n",
    "We first need to import the required libraries for Python:\n",
    "*  *matplotlib.pyplot* (for generating plots)\n",
    "*  *numpy* (for linear algebra routines)\n",
    "*  *math* (for mathematical functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKEmzWP2RLvr",
    "outputId": "68999298-d06e-462c-c6fa-7994e39f3185"
   },
   "outputs": [],
   "source": [
    "## Loading Python libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNFzCEdSRZrM"
   },
   "source": [
    "### 1.2 Which data?\n",
    "\n",
    "We are going to consider a total of (up to) five datasets here:\n",
    "\n",
    "1. Data for a toy model.\n",
    "2. Three datasets for Michaelis-Menten kinetics (low concentration data, high concentration data, and combined low and high concentration data).\n",
    "3. Data for coral reef recovery.\n",
    "\n",
    "The key thing is that we choose below which dataset we are analysing at any given time, by placing this data in the arrays **x_obs** and **y_obs**.\n",
    "\n",
    "We then do a preliminary plot of the data (before fitting it to the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. Data for a toy model\n",
    "# x_obs = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "# y_obs = np.array([0.2, 5, 9, 12, 19, 20, 30, 38, 52, 80, 140])\n",
    "\n",
    "# ## 2.1 Data for Michaelis-Menten kinetics (high concentration data)\n",
    "# x_obs = np.array([1000, 1500, 2000, 2500, 3000])\n",
    "# y_obs = np.array([436.0465, 518.3769, 492.7244, 412.1804, 470.0788])\n",
    "\n",
    "## 2.2 Data for Michaelis-Menten kinetics (low concentration data)\n",
    "x_obs = np.array([10, 20, 30, 40, 50])\n",
    "y_obs = np.array([31.9149, 29.0095, 96.5600, 88.4801, 199.3002])\n",
    "\n",
    "# ## 2.3 Data for Michaelis-Menten kinetics (high and low concentration data)\n",
    "# x_obs = np.array([10, 20, 30, 40, 50, 1000, 1500, 2000, 2500, 3000])\n",
    "# y_obs = np.array([31.9149, 29.0095, 96.5600, 88.4801, 199.3002, \\\n",
    "#                 436.0465, 518.3769, 492.7244, 412.1804, 470.0788])\n",
    "\n",
    "# ## 3. Data for coral reef recovery\n",
    "# x_obs = np.array([0,800,1100,1500,1900,2200,2600,2900,3200,3600,4000])\n",
    "# y_obs = np.array([2,4,8,22,39,59,68,69,74,82,81])\n",
    "\n",
    "## Plot the chosen dataset\n",
    "plt.plot(x_obs, y_obs, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Which model?\n",
    "\n",
    "We are going to consider three different models:\n",
    "\n",
    "#### Toy exponential model for fitting to toy data\n",
    "\n",
    "$y = a e^{b x}$, where: $x$ is model input, $y$ is model output, and there are 2 parameters to estimate: $a$ and $b$.\n",
    "\n",
    "#### Michaelis-Menten kinetics\n",
    "\n",
    "$\\nu = \\dfrac{k_{cat} [E_T] [S]}{K_M + S}$, where: $[S]$ is substrate concentration (model input), $\\nu$ is reaction rate (model output), and there are 3 parameters to estimate: $[E_T]$, $k_{cat}$ and $K_M$.\n",
    "\n",
    "#### Logistic model for coral reef recovery\n",
    "\n",
    "$\\dfrac{\\mathrm{d}C(t)}{\\mathrm{d}t} = rC(t) \\left(1 - \\dfrac{C(t)}{K} \\right), \\,\\, C(0) = C_0$, where $t$ is time since coral reef recovery began (model input), $C(t)$ is coral reef cover at time $t$ (model output), and there are 3 parameters to estimate: $C_0$, $r$ and $K$.\n",
    "\n",
    "#### NOTE \\#1: It's going to be useful here to have *slightly* different syntax for how we code up these models in the Bayesian implementation vs the frequentist implementation in Python.\n",
    "\n",
    "Instead of passing all parameters to our model as separate inputs (e.g. $a$ and $b$ for the toy exponential model, $[E_T]$, $k_{cat}$ and $K_M$ for the Michaelis-Menten kinetics model, etc.), we are going to pass all parameters *together* in a parameter vector $\\theta$. This generalisation will \"future proof\" our code so that it makes things easier later on when we input our model into the Sequential Monte Carlo algorithm!\n",
    "\n",
    "#### NOTE \\#2: We need to use a better implementation of the coral model! The curse of computational cost associated with Bayesian inference approaches hurts us a bit here.\n",
    "\n",
    "When doing the frequentist model-data calibrations, we implemented the coral model using a forward Euler discretisation, without the use of any additional packages. This worked, but it was also inefficient. It will be totally impractical here.\n",
    "\n",
    "Now we use an inbuilt ODE solver for this model within Python, called *odeint*, which is a lot faster. However, it'll still be pretty slow when used within the Bayesian approach... we will talk more about how to resolve this in Section 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Toy exponential model\n",
    "# def y_model_function(x, parameters):\n",
    "#     a = parameters[0]\n",
    "#     b = parameters[1]\n",
    "#     y_model = a*np.exp(b*x)\n",
    "#     return y_model\n",
    "\n",
    "## Michaelis-Menten kinetics model\n",
    "def y_model_function(S, parameters):\n",
    "    E_T = parameters[0]\n",
    "    k_cat = parameters[1]\n",
    "    K_M = parameters[2]\n",
    "    nu = k_cat*E_T*S/(K_M+S)\n",
    "    return nu\n",
    "\n",
    "# ## Coral reef recovery model - this will be TOO SLOW HERE!\n",
    "# def y_model_function(T, parameters):\n",
    "#     C_0 = parameters[0]\n",
    "#     K = parameters[1]\n",
    "#     r = parameters[2]\n",
    "#     vector_of_C_at_times_T = np.zeros(len(T))\n",
    "#     for obs_number in range(len(T)):\n",
    "#         # For each observation, re-run the model\n",
    "#         if T[obs_number] == 0:\n",
    "#             vector_of_C_at_times_T[obs_number] = C_0\n",
    "#             # No need to run the model if we are looking at an observation at t = 0!\n",
    "#         else:      \n",
    "#             approx_dt = 1 # (Approximate) timestep of model\n",
    "#             num_t_values = round(T[obs_number]/approx_dt) # Number of timesteps in the model.\n",
    "#             t = np.linspace(0, T[obs_number], num=num_t_values+1) # Set up the vector for time t.\n",
    "#             dt = t[1]-t[0] # \"Delta t\", i.e. how spaced apart each time value t actually is.\n",
    "#             C = np.zeros(num_t_values+1)\n",
    "#             C[0]=C_0\n",
    "#             for n in range(num_t_values): # Running the ODE model!\n",
    "#                 C[n+1]=C[n] + dt * r * C[n] * (1 - C[n]/K)\n",
    "#             vector_of_C_at_times_T[obs_number] = C[-1]\n",
    "#     return vector_of_C_at_times_T\n",
    "\n",
    "# ## Coral reef recovery model - FASTER code! Now using an in-built ODE solver\n",
    "# from scipy.integrate import odeint\n",
    "# def y_model_function(T, parameters):\n",
    "#     T = np.concatenate(([0],T))\n",
    "#     C_0 = parameters[0]\n",
    "#     K = parameters[1]\n",
    "#     r = parameters[2]\n",
    "#     def ODE_model(C,t):\n",
    "#         dCdt = r*C*(1-C/K)\n",
    "#         return dCdt\n",
    "#     vector_of_C_at_times_T = np.ravel(odeint(ODE_model,C_0,T))\n",
    "#     output_vector_of_C_at_times_T = vector_of_C_at_times_T[1:]\n",
    "#     return output_vector_of_C_at_times_T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Which prior distribution?\n",
    "\n",
    "Recall that for Bayesian inference we also need to choose some prior distributions for our model parameters. In this course we are going to limit our consideration to prior distributions which are uniform in each parameter, i.e. $\\theta_i \\sim \\mathcal{U}(\\theta_{i,\\mathrm{min}},\\theta_{i,\\mathrm{max}})$, but the code below is also easily adaptable to consider other prior distributions as well!\n",
    "\n",
    "#### Wait, we have an additional parameter now: $\\sigma$!\n",
    "In our Bayesian implementations of model-data calibration we will also be estimating the noise parameter $\\sigma$, so we need to define a uniform prior for $\\sigma$ as well. We will always put this parameter **last** in our list of parameters, as this help with generalisation of our code for later on when we are doing Sequential Monte Carlo!\n",
    "\n",
    "Hence, compared to our frequentist implementations of model-data calibration, our Bayesian implementations of model-data calibration are technically estimating *one additional parameter* ($\\sigma$)!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. Suggested prior distribution bounds for toy exponential model\n",
    "# theta_min = np.array([0,0,0])  # Lower bounds on parameters a, b and sigma\n",
    "# theta_max = np.array([5,5,10]) # Upper bounds on parameters a, b and sigma\n",
    "\n",
    "## 2. Suggested prior distribution bounds for Michaelis-Menten kinetics models\n",
    "theta_min = np.array([0, 0,  0,  0])     # Lower bounds on parameters [E_T], k_cat, K_M and sigma\n",
    "theta_max = np.array([50,1000,1500,500]) # Upper bounds on parameters [E_T], k_cat, K_M and sigma\n",
    "\n",
    "# ## 3. Suggested prior distribution bounds for coral reef recovery model\n",
    "# theta_min = np.array([0,0,0,0])        # Lower bounds on parameters C_0, K, r and sigma\n",
    "# theta_max = np.array([5,100,0.01,100]) # Upper bounds on parameters C_0, K, r and sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 OK We are ready to turn the handle on the Sequential Monte Carlo algorithm.\n",
    "\n",
    "The code block below implements a Python implementation of the Sequential Monte Carlo algorithm used in [these](https://www.science.org/doi/10.1126/sciadv.abm5952) [three](https://onlinelibrary.wiley.com/doi/full/10.1111/ele.13465) [papers](https://www.sciencedirect.com/science/article/pii/S1364815220300827). It is not super-fast, but it does work!\n",
    "\n",
    "There are some tuning parameters associated with this Sequential Monte Carlo algorithm that you can change if you need to:\n",
    "* The number of particles $M>0$. Increase this number for higher accuracy but slower speed.\n",
    "* The effective sample size reduction target $\\Delta>0$. Decrease this number for higher accuracy but slower speed.\n",
    "* The particle mutation fraction $0<C<1$. Increase this number for higher accuracy but slower speed.\n",
    "\n",
    "For some of the later model-data fits, you **will likely need to modify these tuning parameters** a bit to get the algorithm to run within a reasonable time (e.g. reduce $M$). However, be aware of the potential accuracy loss. You'll be able to check whether your modification of tuning parameters affected your results when you plot your model-data fits (see Section 1.6).\n",
    "\n",
    "**You shouldn't need to modify anything else** in the code block in this course. However, feel free to ask questions if you are curious about what's going on in this code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PsvRl_K2RtS2",
    "outputId": "a5b48f24-ef77-47e7-acbf-6a1333220a92"
   },
   "outputs": [],
   "source": [
    "# 1. Sequential Monte Carlo algorithm tuning parameters.\n",
    "M = 1000\n",
    "Delta = 0.05\n",
    "C = 0.95\n",
    "\n",
    "# 2. Define logprior distribution. Change this if you are not using uniform priors.\n",
    "def logprior_distribution(theta,theta_min,theta_max):\n",
    "    logprior_for_theta = 0\n",
    "    for i in range(len(theta_min)):\n",
    "        if theta[i] > theta_min[i] and theta[i] < theta_max[i]:\n",
    "            logprior_for_theta = logprior_for_theta + np.log(1/(theta_max[i]-theta_min[i]))\n",
    "        else:\n",
    "            logprior_for_theta = logprior_for_theta - np.inf\n",
    "    return logprior_for_theta\n",
    "\n",
    "# 3. Sample from the prior distribution. Change this if you are not using uniform priors.\n",
    "def sample_from_prior_distribution(theta_min,theta_max):\n",
    "  theta = np.zeros((len(theta_min),1))\n",
    "  for i in range(len(theta_min)):\n",
    "    theta[i] = theta_min[i] + (theta_max[i] - theta_min[i]) * np.random.rand()\n",
    "  return theta\n",
    "\n",
    "# 4. Define loglikelihood function. Change this if you are not using a Gaussian likelihood function.\n",
    "def loglikelihood_function(func, x_obs, y_obs, theta):\n",
    "  params = theta[0:-1]\n",
    "  sigma = theta[-1]\n",
    "  y_model = func(x_obs,params)\n",
    "  N_obs = len(y_obs)\n",
    "  Loglikelihood = -0.5*N_obs*np.log(2*math.pi) \\\n",
    "                  - 0.5*N_obs*np.log( np.square(sigma)) \\\n",
    "                  - 0.5 * np.sum( np.square((y_obs-y_model)/sigma) )\n",
    "  return Loglikelihood\n",
    "\n",
    "# 4. Generate M prior samples.\n",
    "N_theta = len(theta_min)\n",
    "theta_samples = np.zeros((N_theta,M))\n",
    "theta_sample_weights = np.ones(M) / M\n",
    "theta_sample_logweights = np.log(theta_sample_weights)\n",
    "theta_sample_loglikelihoods = np.zeros(M)\n",
    "for m in range(M):\n",
    "    theta_samples[:,[m]] = sample_from_prior_distribution(theta_min,theta_max)\n",
    "    theta_sample_loglikelihoods[m] = \\\n",
    "    loglikelihood_function(y_model_function, x_obs, y_obs, theta_samples[:,[m]])\n",
    "prior_samples = np.copy(theta_samples)\n",
    "\n",
    "# 5. Run the SMC algorithm\n",
    "gamma=0\n",
    "ESS=M\n",
    "SMC_complete = False\n",
    "while True:\n",
    "  # 5.1 Update the value of gamma, weights, logweights and ESS.\n",
    "  ESS_target = ESS*(1-Delta)\n",
    "  gamma_trial = 1\n",
    "  while True:\n",
    "    theta_sample_logweights_trial = theta_sample_logweights + (gamma_trial-gamma) * theta_sample_loglikelihoods\n",
    "    theta_sample_logweights_trial = theta_sample_logweights_trial - max(theta_sample_logweights_trial)\n",
    "    # Note that these logweights are not normalised. This is intentional to avoid floating point errors.\n",
    "\n",
    "    theta_sample_weights_trial = np.exp(theta_sample_logweights_trial)\n",
    "    theta_sample_weights_trial = theta_sample_weights_trial/np.sum(theta_sample_weights_trial)\n",
    "    # However, weights ARE normalised. This is necessary for proper calculation of ESS.\n",
    "    ESS_trial = 1/np.sum(np.square(theta_sample_weights_trial))\n",
    "\n",
    "    if gamma_trial == 1:\n",
    "      if ESS_trial >= ESS_target:\n",
    "        # Our sample is good enough to be the final sample from the posterior!\n",
    "        SMC_complete = True\n",
    "        break\n",
    "      else:\n",
    "        # Otherwise, start the bisection method to obtain the new value of gamma < 1.\n",
    "        gamma_lower_guess = np.copy(gamma)\n",
    "        gamma_upper_guess = 1\n",
    "    else:\n",
    "      if abs(ESS_trial-ESS_target)/ESS_target < 1e-6:\n",
    "        # We found the next value of gamma!\n",
    "        break\n",
    "      else:\n",
    "        if ESS_trial > ESS_target:\n",
    "          gamma_lower_guess = np.copy(gamma_trial)\n",
    "        else:\n",
    "          gamma_upper_guess = np.copy(gamma_trial)\n",
    "    gamma_trial = (gamma_upper_guess+gamma_lower_guess)/2\n",
    "\n",
    "  gamma=np.copy(gamma_trial)\n",
    "  ESS=np.copy(ESS_trial)\n",
    "  theta_sample_weights=np.copy(theta_sample_weights_trial)\n",
    "  theta_sample_logweights=np.copy(theta_sample_logweights_trial)\n",
    "  print(f\"SMC algorithm is in progress: The current value of gamma is\",gamma,\". Algorithm concludes when gamma = 1.\")\n",
    "\n",
    "  # 5.2 If gamma = 1, complete the SMC algorithm by performing one final\n",
    "  # resampling step.\n",
    "  if SMC_complete == True:\n",
    "    resample_columns = np.random.choice(M,M,p=theta_sample_weights)\n",
    "    theta_samples_trials = np.zeros((N_theta,M))\n",
    "    for m in range(M):\n",
    "      theta_samples_trials[:,[m]]=theta_samples[:,[resample_columns[m]]]\n",
    "    theta_samples = np.copy(theta_samples_trials)\n",
    "    theta_sample_weights = np.ones(M) / M\n",
    "    theta_sample_logweights = np.log(theta_sample_weights)\n",
    "    ESS=np.copy(M)\n",
    "    break\n",
    "\n",
    "  # 5.3 If gamma < 1 and ESS < M/2, resample.\n",
    "  if ESS < M/2:\n",
    "    resample_columns = np.random.choice(M,M,p=theta_sample_weights)\n",
    "    theta_samples_trials = np.zeros((N_theta,M))\n",
    "    for m in range(M):\n",
    "      theta_samples_trials[:,[m]]=theta_samples[:,[resample_columns[m]]]\n",
    "    theta_samples = np.copy(theta_samples_trials)\n",
    "    theta_sample_weights = np.ones(M) / M\n",
    "    theta_sample_logweights = np.log(theta_sample_weights)\n",
    "    ESS=np.copy(M)\n",
    "\n",
    "  # 5.4 Mutation step\n",
    "  r=0\n",
    "  particle_mutation_vector = np.zeros(M)\n",
    "  while True:\n",
    "    r=r+1\n",
    "    theta_mean = np.zeros((N_theta,1))\n",
    "    for n_theta in range(N_theta):\n",
    "      theta_mean[n_theta,0] = np.sum(theta_sample_weights*theta_samples[n_theta,:])\n",
    "    covariance_matrix = np.zeros((N_theta,N_theta))\n",
    "    for m in range(M):\n",
    "      covariance_matrix = covariance_matrix + theta_sample_weights[m] * \\\n",
    "      (theta_samples[:,[m]]-theta_mean) * np.transpose(theta_samples[:,[m]]-theta_mean)\n",
    "    covariance_matrix = covariance_matrix / (1-np.sum(np.square(theta_sample_weights)))\n",
    "    # Calculate an empirical covariance matrix for the proposal distribution.\n",
    "\n",
    "    for m in range(M):\n",
    "      log_intermediate_theta_current = logprior_distribution(theta_samples[:,[m]],theta_min,theta_max) + theta_sample_loglikelihoods[m] * gamma\n",
    "      # Calculate current value of log-intermediate-distribution for theta.\n",
    "\n",
    "      theta_star = np.transpose(np.random.multivariate_normal(np.ravel(theta_samples[:,[m]]),covariance_matrix/np.square(r),1))\n",
    "      theta_star_loglikelihood = loglikelihood_function(y_model_function, x_obs, y_obs, theta_star)\n",
    "      log_intermediate_theta_star = logprior_distribution(theta_star,theta_min,theta_max) + theta_star_loglikelihood * gamma\n",
    "      # Calculate value of log-intermediate-distribution for proposed new particle location theta_star.\n",
    "\n",
    "      if min(1, np.exp(log_intermediate_theta_star-log_intermediate_theta_current)) > np.random.rand():\n",
    "        theta_samples[:,[m]] = theta_star\n",
    "        theta_sample_loglikelihoods[m] = theta_star_loglikelihood\n",
    "        particle_mutation_vector[m] = 1\n",
    "\n",
    "    if np.sum(particle_mutation_vector)/M >= C:\n",
    "      break\n",
    "      # Exit mutation step since a sufficient number of particles moved.\n",
    "posterior_samples = np.copy(theta_samples)\n",
    "\n",
    "print(\"The Sequential Monte Carlo algorithm estimates that the mean values of your model parameters are:\")\n",
    "print(np.mean(posterior_samples,axis=1))\n",
    "print(\"The Sequential Monte Carlo algorithm estimates that the standard deviations of your model parameters are:\")\n",
    "print(np.std(posterior_samples,axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Plot the model-data fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_model = np.linspace(min(x_obs),max(x_obs),101)\n",
    "y_model_all_samples = np.zeros((len(x_model),M))\n",
    "for m in range(M):\n",
    "    y_model_all_samples[:,[m]] = np.reshape(y_model_function(x_model, posterior_samples[:,[m]]), (len(x_model),1))\n",
    "    # Reshape needed to convert 1D array to 2D array\n",
    "plt.plot(x_model,y_model_all_samples,'r')\n",
    "plt.plot(x_obs, y_obs, 'ok')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Visualisation\n",
    "\n",
    "In this module, we are going to focus on visualising our calibration outputs.\n",
    "\n",
    "We have already seen in Section 1.5 how to plot the $M=1000$ samples of our posterior distribution.\n",
    "\n",
    "We will do five types of visualisation in this module (2.1-2.5), and will need Bayesian model-data calibration outputs for four of them: **2.2** Median and credible interval predictions, **2.3** Parameter histograms, **2.4** Marginal distributions, and **2.5** Bivariate scatter plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Median and credible interval predictions\n",
    "\n",
    "First, let's calculate the median, and 68% and 95% central credible intervals, for the *uncertainty in the best-fit predictions*, i.e. uncertainty in $\\mathbf{y}_{\\mathrm{model}}(\\mathbf{x},\\mathbf{\\theta})$.\n",
    "\n",
    "This is pretty straightforward, since we have already plotted the predictions of all (equally-weighted) samples in Section 1.5.\n",
    "\n",
    "All we need to do after that is find the 2.5th, 16th, 50th, 84th and 97.5th percentiles across these predictions to get the desired median and credible intervals!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model_percentiles = np.zeros((len(x_model),5))\n",
    "for x in range(len(x_model)):\n",
    "    y_model_percentiles[x,:]=np.percentile(y_model_all_samples[x,:],[2.5,16,50,84,97.5])\n",
    "plt.fill_between(x_model,\n",
    "                 np.ravel(y_model_percentiles[:,[4]]), # 97.5th percentile\n",
    "                 np.ravel(y_model_percentiles[:,[0]]), # 2.5th percentile\n",
    "                 color=\"lightblue\",\n",
    "                 label='95% credible interval')\n",
    "plt.fill_between(x_model,\n",
    "                 np.ravel(y_model_percentiles[:,[3]]), # 84th percentile\n",
    "                 np.ravel(y_model_percentiles[:,[1]]), # 16th percentile\n",
    "                 color=\"deepskyblue\",\n",
    "                 label='68% credible interval')\n",
    "plt.plot(x_model,y_model_percentiles[:,[2]],'b',label='Median') # 50th percentile\n",
    "plt.plot(x_obs,y_obs,'ok')\n",
    "plt.title('Uncertainty in the best-fit predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Median and credible interval predictions (continued)\n",
    "\n",
    "Now, let's calculate the median, and 68% and 95% central credible intervals, for the *uncertainty in new observations*, i.e. uncertainty in $\\mathbf{y}_{\\mathrm{obs}}$.\n",
    "\n",
    "This just involves sampling **also** from the error in the model-data fit, $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$, and adding this sampled value $\\varepsilon$ to our predictions of $\\mathbf{y}_{\\mathrm{model}}(\\mathbf{x},\\mathbf{\\theta})$ from all (equally-weighted) samples.\n",
    "\n",
    "We then find the 2.5th, 16th, 50th, 84th and 97.5th percentiles, in the same way as before.\n",
    "\n",
    "These new credible intervals (to be plotted below) account for our estimated values of $\\sigma$, whereas the credible intervals plotted above completely ignore $\\sigma$.\n",
    "\n",
    "You should expect, for example, to see $\\approx$68% of the data used to fit the model falling within the dark blue shaded region, and $\\approx$95% of the data used to fit the model falling within the light blue shaded region!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new_observations = np.zeros((len(x_model),M))\n",
    "for m in range(M):\n",
    "    y_new_observations[:,[m]] = y_model_all_samples[:,[m]] + np.random.normal(0,posterior_samples[-1,m])\n",
    "\n",
    "y_percentiles_new_observations = np.zeros((len(x_model),5))\n",
    "for x in range(len(x_model)):\n",
    "    y_percentiles_new_observations[x,:]=np.percentile(y_new_observations[x,:],[2.5,16,50,84,97.5])\n",
    "\n",
    "plt.fill_between(x_model,\n",
    "                 np.ravel(y_percentiles_new_observations[:,[4]]), # 97.5th percentile\n",
    "                 np.ravel(y_percentiles_new_observations[:,[0]]), # 2.5th percentile\n",
    "                 color=\"lightblue\",\n",
    "                 label='95% credible interval')\n",
    "plt.fill_between(x_model,\n",
    "                 np.ravel(y_percentiles_new_observations[:,[3]]), # 84th percentile\n",
    "                 np.ravel(y_percentiles_new_observations[:,[1]]), # 16th percentile\n",
    "                 color=\"deepskyblue\",\n",
    "                 label='68% credible interval')\n",
    "plt.plot(x_model,y_percentiles_new_observations[:,[2]],'b',label='Median') # 50th percentile\n",
    "plt.plot(x_obs,y_obs,'ok')\n",
    "plt.title('Uncertainty in new observations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Parameter histograms\n",
    "\n",
    "Exactly what it says on the tin. Let's make some histograms!\n",
    "\n",
    "**Note:** We now should define some labels for our parameters, to make it easier to interpret our histograms (and some of the later plots as well!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter_labels = ['$a$','$b$','$\\sigma$']                  # Labels for figures from the toy exponential model\n",
    "Parameter_labels = ['$E_T$','$k_{cat}$','$K_M$','$\\sigma$'] # Labels for figures from the Michaelis-Menten kinetics models\n",
    "#Parameter_labels = ['$C_0$','$K$','$r$','$\\sigma$']         # Labels for figures from the coral reef recovery model\n",
    "\n",
    "\n",
    "for n in range(N_theta): # For all fitted parameters N_theta\n",
    "    plt.hist(posterior_samples[n,:]) # HISTOGRAM!\n",
    "    plt.xlim([theta_min[n],theta_max[n]]) # Include this code line if you want to use the prior bounds as left- and\n",
    "                                          # right-boundaries of your histograms\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel(r'Value of parameter ' + Parameter_labels[n])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Marginal distributions\n",
    "\n",
    "First, let's consider **one** parameter $a$ from the toy exponental model, and compare its:\n",
    "1. Histogram,\n",
    "2. Kernel density estimation (KDE) - the no-frills version,\n",
    "3. Kernel density estimation (KDE) with reflection boundary correction.\n",
    "\n",
    "This will give us a good sense of what the KDE is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import stats\n",
    "!pip install kalepy    \n",
    "import kalepy as kale   \n",
    "# You'll only need to run these 3 code lines once at most, and then you can comment them out!\n",
    "\n",
    "# a_samples = posterior_samples[0,:]\n",
    "\n",
    "# ## 1. Histogram\n",
    "# fig, axs = plt.subplots(1, 2)\n",
    "# axs[0].hist(a_samples)\n",
    "# axs[0].set_ylabel('Frequency')\n",
    "# axs[0].set_xlabel(r'Value of parameter $a$')\n",
    "# plt.title(r'Comparing methods of showing the probability distribution for parameter $a$')\n",
    "\n",
    "# ## 2. Kernel density estimation (KDE), no frills\n",
    "# a_KDE_model = stats.gaussian_kde(a_samples)\n",
    "# a_vec = np.linspace(min(a_samples),max(a_samples),1001)\n",
    "# a_KDE_vec = a_KDE_model(a_vec)\n",
    "# axs[1].plot(a_vec,a_KDE_vec,'k', label='KDE with no modifications')\n",
    "\n",
    "# # ## 2. Kernel density estimation (KDE), no frills - ALTERNATIVE PACKAGE\n",
    "# # a_vec_alt,a_KDE_vec_alt = kale.density(a_samples,probability=True)\n",
    "# # axs[1].plot(a_vec_alt,a_KDE_vec_alt,'--b')\n",
    "\n",
    "# ## 3. Kernel density estimation (KDE) with reflection boundary correction\n",
    "# a_vec_reflect, a_KDE_vec_reflect = \\\n",
    "#     kale.density(a_samples,reflect=[theta_min[0],theta_max[0]],probability=True)\n",
    "# axs[1].plot(a_vec_reflect,a_KDE_vec_reflect,'--r',label = 'KDE with reflecting boundary')\n",
    "\n",
    "# ## Some additional code to make the plot prettier\n",
    "# axs[1].set_xlabel(r'Value of parameter $a$')\n",
    "# axs[1].yaxis.set_label_position('right')\n",
    "# axs[1].set_ylabel('Probability density')\n",
    "# plt.legend(bbox_to_anchor=(1.05,1))\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Marginal distributions (continued)\n",
    "\n",
    "Now let's do kernel density estimation with reflection boundary condition for *all* parameters of the model!\n",
    "\n",
    "This also provides a good opportunity to compare the *best-fit* parameter estimates we got from the frequentist model-data calibration to the *marginal distributions* we obtain from the Bayesian model-data calibration.\n",
    "\n",
    "To do this, have a quick look over at **Section 2.4 of the frequentist Python code** to extract $a$, $b$ and RMSE, and put their values in below! (A good ol' fashioned Ctrl+C may be your friend here!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_T_MLE=8535.09375\n",
    "k_cat_MLE=8594.85054\n",
    "K_M= 23321286.7\n",
    "RMSE= 37.92390675220631\n",
    "\n",
    "theta_MLE = [E_T_MLE,k_cat_MLE,K_M,RMSE]\n",
    "for n in range(N_theta): # For all fitted parameters N_theta\n",
    "    param_vec_reflect, param_KDE_vec_reflect = \\\n",
    "        kale.density(posterior_samples[n,:],reflect=[theta_min[n],theta_max[n]],probability=True)\n",
    "    plt.plot(param_vec_reflect,param_KDE_vec_reflect,label='Marginal density from Bayesian inference')\n",
    "    plt.plot([theta_MLE[n],theta_MLE[n]],[0,max(param_KDE_vec_reflect)],label='Best fit estimate from maximum likelihood estimation')\n",
    "    plt.xlabel(r'Value of parameter ' + Parameter_labels[n])\n",
    "    plt.ylabel('Probability density')\n",
    "    plt.legend(bbox_to_anchor=(1.05,1))\n",
    "    plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Bivariate scatter plots\n",
    "\n",
    "We plot all parameters against each other in two dimensions, to compare pairs of parameters with each other, $\\theta_i$ vs $\\theta_j$.\n",
    "\n",
    "**Note:** It's common to exclude parameters that quantify noise in these comparisons (i.e. we ignore $\\sigma$), so that we are just focusing on comparisons between parameters that control the *deterministic* behaviour of the model (rather than the statistical distribution around it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(N_theta-1): # Exclude sigma\n",
    "    for j in range(i+1,N_theta-1): # Exclude sigma\n",
    "        plt.plot(prior_samples[i,:], prior_samples[j,:], 'oy'); # If we want to plot prior samples too for comparison!\n",
    "        plt.plot(posterior_samples[i,:], posterior_samples[j,:], 'ob');\n",
    "        plt.xlabel(r'Value of parameter ' + Parameter_labels[i])\n",
    "        plt.ylabel(r'Value of parameter ' + Parameter_labels[j])\n",
    "        #plt.xlim(theta_min[i],theta_max[i]) # If we want the bounds of our bivariate scatter plots to match our prior bounds\n",
    "        #plt.ylim(theta_min[j],theta_max[j]) # If we want the bounds of our bivariate scatter plots to match our prior bounds\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analysis of model slopppiness\n",
    "\n",
    "In this module we are going to focus on the task of calculating the eigenparameters (parameter combinations) of the model, ordered from stiffest (most sensitive to model-data fit) to sloppiest (least sensitive to model-data fit).\n",
    "\n",
    "For Bayesian model-data calibrations, we will discuss two sensitivity matrices: the PCA Hessian matrix **P** and the likelihood-informed subspace (LIS)-based sensitivity matrix **G**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 PCA Hessian matrix\n",
    "\n",
    "Let's start by doing analysis of sloppiness using the *PCA Hessian matrix* **P** as the sensitivity matrix. Differently to the sensitivity matrices used for frequentist model-data calibration (which only considered *infinitesimal* variations of the likelihood around the MLE), the PCA Hessian matrix **P** considers the *entire posterior distribution* (as approximated by the samples that we obtained from Sequential Monte Carlo).\n",
    "\n",
    "The code below can be changed to any of the model examples, by simply:\n",
    "* Changing the parameter names.\n",
    "\n",
    "The code block ends by printing out the eigenparameters, ordered from stiffest to sloppiest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter_names = ['a','b']              # Ordered parameter names for the toy exponential model\n",
    "Parameter_names = ['E_T','k_cat','K_M'] # Ordered parameter names for the Michaelis-Menten kinetics models\n",
    "#Parameter_names = ['C_0','K','r']       # Ordered parameter names for the coral reef recovery model\n",
    "\n",
    "log_theta = np.log(posterior_samples[0:-1,:])\n",
    "# Calculate log-parameters, excluding sigma.\n",
    "\n",
    "mean_log_theta = np.reshape(np.mean(log_theta,axis=1),(N_theta-1,1))\n",
    "# Calculate the estimated posterior mean for the natural logarithm of parameters.\n",
    "\n",
    "sample_covariance = np.zeros((N_theta-1,N_theta-1))\n",
    "for m in range(M):\n",
    "    sample_covariance = sample_covariance + \\\n",
    "    (log_theta[:,[m]] - mean_log_theta) * np.transpose(log_theta[:,[m]] - mean_log_theta)\n",
    "sample_covariance = sample_covariance/(M-1)\n",
    "# Calculate the sample covariance matrix for the posterior distribution with respect to log-parameters.\n",
    "\n",
    "# Obtain the PCA Hessian matrix P!\n",
    "P = np.linalg.inv(sample_covariance)\n",
    "\n",
    "lamda, v = np.linalg.eig(P)\n",
    "# Perform eigendecomposition on the PCA Hessian matrix P!\n",
    "\n",
    "reordered_lamda_elements = lamda.argsort()[::-1]\n",
    "lamda = lamda[reordered_lamda_elements]\n",
    "v = v[:,reordered_lamda_elements]\n",
    "# Reorder eigenvalues from largest to smallest, and reorder the eigenvectors accordingly\n",
    "\n",
    "lamda = lamda/max(lamda)\n",
    "# Rescale the eigenvalues.\n",
    "\n",
    "for n in range(N_theta-1):\n",
    "    if abs(min(v[:,n])) > max(v[:,n]):\n",
    "        v[:,n]=v[:,n]/min(v[:,n])\n",
    "    else:\n",
    "        v[:,n]=v[:,n]/max(v[:,n])\n",
    "# Rescale each eigenvector so that the leading parameter within each eigenvector has index of +1\n",
    "\n",
    "for j in range(N_theta-1):\n",
    "    Eigenparameter = Parameter_names[0] + '^' + str(round(v[0,j],2))\n",
    "    for i in range(1,N_theta-1):\n",
    "        Eigenparameter = Eigenparameter + ' x ' + Parameter_names[i] + '^' + str(round(v[i,j],2))\n",
    "    print(f'Eigenparameter',j+1,'is',Eigenparameter,'corresponding to lambda_j/lambda_1=',round(lamda[j],3))\n",
    "# Report the eigenparameters!\n",
    "# Note we don't bother here to remove parameters with indices between -0.2 and +0.2, I leave that you to interpret correctly! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Likelihood-informed subspace (LIS)-based sensitivity matrix G\n",
    "\n",
    "We can alternatively use the likelihood-informed subspace (LIS)-based sensitivity matrix **G** as the sensitivity matrix. This is by far the most complicated sensitivity matrix to compute, of the four we will consider in this course, and it has an equally complicated interpretation: The LIS-based sensitivity matrix **G** considers the *entire posterior distribution* but attempts to *eliminate the effects of the prior distribution* so that the sensitivity matrix which is obtained is based, as much as possible, only on the data. To obtain **G**, we need to compute the Hessian matrix individually for *every posterior sample*, which is a slow process (although nowhere near as slow as Sequential Monte Carlo sampling!). To speed this up in the code below we have used the Levenberg-Marquardt Hessian matrix as an approximation of the Hessian matrix.\n",
    "\n",
    "The code below assumes you have already run the code in Section 3.1 (and thus you already have set the parameter names to suit the model you are analysing).\n",
    "\n",
    "The code block ends by printing out the eigenparameters, ordered from stiffest to sloppiest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Calculate the covariance matrix Omega of the prior distribution\n",
    "log_prior_theta = np.log(prior_samples[0:-1,:])\n",
    "# Calculate log-parameters of the prior, excluding sigma.\n",
    "\n",
    "mean_log_prior_theta = np.reshape(np.mean(log_prior_theta,axis=1),(N_theta-1,1))\n",
    "# Calculate the estimated prior mean for the natural logarithm of parameters.\n",
    "\n",
    "sample_prior_covariance = np.zeros((N_theta-1,N_theta-1))\n",
    "for m in range(M):\n",
    "    sample_prior_covariance = sample_prior_covariance + \\\n",
    "    (log_prior_theta[:,[m]] - mean_log_prior_theta) * np.transpose(log_prior_theta[:,[m]] \\\n",
    "    - mean_log_prior_theta)\n",
    "sample_prior_covariance = sample_prior_covariance/(M-1)\n",
    "# Calculate the sample covariance matrix for the prior distribution with respect to log-parameters.\n",
    "\n",
    "# Step 2: Use Cholesky decomposition to obtain L_p\n",
    "L_p = np.linalg.cholesky(sample_prior_covariance)\n",
    "\n",
    "\n",
    "def r_function(x_obs, y_obs, k, theta):\n",
    "  sigma = 1 # Again it doesn't actually matter what the value of sigma is! (As long as it's positive.)\n",
    "  y_model_k = y_model_function([x_obs[k]],theta)\n",
    "  r_k_theta = (y_obs[k]-y_model_k)/sigma\n",
    "  return r_k_theta\n",
    "\n",
    "# Step 3: Calculate the Hessian matrix for all posterior samples (let's do the \"fast\" version)\n",
    "def calculate_Levenberg_Marquardt_Hessian(theta_star,loglikelihood_function,y_model_function,x_obs,y_obs,delta):\n",
    "    N_theta = len(theta_star) \n",
    "    L = np.zeros((N_theta-1,N_theta-1))\n",
    "    # This subtraction of one is to avoid sigma being part of the sensitivity matrix\n",
    "    logL_star = loglikelihood_function(y_model_function,x_obs,y_obs,theta_star)\n",
    "    \n",
    "    # Define common things needed to calculate L\n",
    "    delta = 1e-4\n",
    "    N_obs = len(y_obs)\n",
    "    dr_dlogtheta = np.zeros((N_obs,N_theta-1))\n",
    "    \n",
    "    # First, calculate all required derivatives d(r_k)/d(log theta_i)\n",
    "    for i in range(N_theta-1):\n",
    "        theta_up = np.copy(theta_star)\n",
    "        theta_up[i] = theta_star[i]*(1 + delta/2)\n",
    "        theta_down = np.copy(theta_star)\n",
    "        theta_down[i] = theta_star[i]*(1 - delta/2)\n",
    "        for k in range(N_obs):\n",
    "            dr_dlogtheta[k,i] = (r_function(x_obs,y_obs,k,theta_up) \\\n",
    "                                 - r_function(x_obs,y_obs,k,theta_down)) / delta\n",
    "\n",
    "    # Second, calculate all elements of Levenberg-Marquardt Hessian matrix L\n",
    "    #for i in range(N_theta):\n",
    "    for i in range(N_theta-1):\n",
    "        for j in range(N_theta-1):\n",
    "            for k in range(N_obs):\n",
    "                L[i,j] = L[i,j] + dr_dlogtheta[k,i]*dr_dlogtheta[k,j]\n",
    "    return L\n",
    "\n",
    "\n",
    "delta = 1e-4\n",
    "G = np.zeros((N_theta - 1,N_theta - 1)) # This subtraction of one is to avoid sigma being part of G\n",
    "for m in range(M):\n",
    "    L = calculate_Levenberg_Marquardt_Hessian(posterior_samples[:,[m]],loglikelihood_function,y_model_function,x_obs,y_obs,delta)\n",
    "    \n",
    "    # Step 4: Calculate the associated prior-preconditioned matrix Psi\n",
    "    Psi = np.matmul(np.matmul(np.transpose(L_p),L),L_p)\n",
    "    \n",
    "    # Step 5: Estimate the LIS-based sensitivity matrix G!\n",
    "    G = G + Psi\n",
    "G = G/M\n",
    "\n",
    "lamda, v = np.linalg.eig(G)\n",
    "# Perform eigendecomposition on the LIS-based sensitivity matrix G!\n",
    "\n",
    "reordered_lamda_elements = lamda.argsort()[::-1]\n",
    "lamda = lamda[reordered_lamda_elements]\n",
    "v = v[:,reordered_lamda_elements]\n",
    "# Reorder eigenvalues from largest to smallest, and reorder the eigenvectors accordingly\n",
    "\n",
    "lamda = lamda/max(lamda)\n",
    "# Rescale the eigenvalues.\n",
    "\n",
    "for n in range(N_theta-1):\n",
    "    if abs(min(v[:,n])) > max(v[:,n]):\n",
    "        v[:,n]=v[:,n]/min(v[:,n])\n",
    "    else:\n",
    "        v[:,n]=v[:,n]/max(v[:,n])\n",
    "# Rescale each eigenvector so that the leading parameter within each eigenvector has index of +1\n",
    "\n",
    "for j in range(N_theta-1):\n",
    "    Eigenparameter = Parameter_names[0] + '^' + str(round(v[0,j],2))\n",
    "    for i in range(1,N_theta-1):\n",
    "        Eigenparameter = Eigenparameter + ' x ' + Parameter_names[i] + '^' + str(round(v[i,j],2))\n",
    "    print(f'Eigenparameter',j+1,'is',Eigenparameter,'corresponding to lambda_j/lambda_1=',round(lamda[j],3))\n",
    "# Report the eigenparameters!\n",
    "# Note we don't bother here to remove parameters with indices between -0.2 and +0.2, I leave that you to interpret correctly! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
