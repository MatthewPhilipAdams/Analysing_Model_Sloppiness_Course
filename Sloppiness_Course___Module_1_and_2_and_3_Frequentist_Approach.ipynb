{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODc4i6TgeCGA"
   },
   "source": [
    "# 1. Model-data calibration\n",
    "In this module, we are going to be focusing on coding up model-data calibration using Bayesian and frequentist approaches.\n",
    "\n",
    "In this particular Python script, we are working on the **frequentist** approach, for 3 model examples:\n",
    "1.    A toy example (fitting an exponential model to some toy data).\n",
    "2.    Michaelis-Menten kinetics for chemical reactions (as in [Monsalve-Bravo *et al.* 2022 Sci. Adv.](https://www.science.org/doi/10.1126/sciadv.abm5952)).\n",
    "3.    Logistic growth model for coral reef recovery (as in [Simpson *et al.* 2022 J. Theor. Biol.](https://www.sciencedirect.com/science/article/pii/S0022519321004185))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgnRLY-FfFus"
   },
   "source": [
    "### 1.1 Preliminary code to run!\n",
    "We first need to import the required libraries for Python:\n",
    "*  *matplotlib.pyplot* (for generating plots)\n",
    "*  *numpy* (for linear algebra routines)\n",
    "*  *lmfit* (for least-squares regression routines)\n",
    "*  *math* (for mathematical functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8KDPcmDreseu",
    "outputId": "44fc2a1f-c8ed-4ff9-ac19-b9be454fc1f0"
   },
   "outputs": [],
   "source": [
    "## Loading Python libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "!pip install lmfit\n",
    "# Note: This \"lmfit\" installation step only needs to be done once\n",
    "from lmfit import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etGI-kb4eFL1"
   },
   "source": [
    "### 1.2 Which data?\n",
    "\n",
    "We are going to consider a total of (up to) five datasets here:\n",
    "\n",
    "1. Data for a toy model.\n",
    "2. Three datasets for Michaelis-Menten kinetics (low concentration data, high concentration data, and combined low and high concentration data).\n",
    "3. Data for coral reef recovery.\n",
    "\n",
    "The key thing is that we choose below which dataset we are analysing at any given time, by placing this data in the arrays **x_obs** and **y_obs**.\n",
    "\n",
    "We then do a preliminary plot of the data (before fitting it to the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Data for a toy model\n",
    "x_obs = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "y_obs = np.array([0.2, 5, 9, 12, 19, 20, 30, 38, 52, 80, 140])\n",
    "\n",
    "# ## 2.1 Data for Michaelis-Menten kinetics (high concentration data)\n",
    "# x_obs = np.array([1000, 1500, 2000, 2500, 3000])\n",
    "# y_obs = np.array([436.0465, 518.3769, 492.7244, 412.1804, 470.0788])\n",
    "\n",
    "# ## 2.2 Data for Michaelis-Menten kinetics (low concentration data)\n",
    "# x_obs = np.array([10, 20, 30, 40, 50])\n",
    "# y_obs = np.array([31.9149, 29.0095, 96.5600, 88.4801, 199.3002])\n",
    "\n",
    "# ## 2.3 Data for Michaelis-Menten kinetics (high and low concentration data)\n",
    "# x_obs = np.array([10, 20, 30, 40, 50, 1000, 1500, 2000, 2500, 3000])\n",
    "# y_obs = np.array([31.9149, 29.0095, 96.5600, 88.4801, 199.3002, \\\n",
    "#                 436.0465, 518.3769, 492.7244, 412.1804, 470.0788])\n",
    "\n",
    "# ## 3. Data for coral reef recovery\n",
    "# x_obs = np.array([0,800,1100,1500,1900,2200,2600,2900,3200,3600,4000])\n",
    "# y_obs = np.array([2,4,8,22,39,59,68,69,74,82,81])\n",
    "\n",
    "## Plot the chosen dataset\n",
    "plt.plot(x_obs, y_obs, 'o')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Which model?\n",
    "\n",
    "We are going to consider three different models:\n",
    "\n",
    "#### Toy exponential model for fitting to toy data\n",
    "\n",
    "$y = a e^{b x}$, where: $x$ is model input, $y$ is model output, and there are 2 parameters to estimate: $a$ and $b$.\n",
    "\n",
    "#### Michaelis-Menten kinetics\n",
    "\n",
    "$\\nu = \\dfrac{k_{cat} [E_T] [S]}{K_M + S}$, where: $[S]$ is substrate concentration (model input), $\\nu$ is reaction rate (model output), and there are 3 parameters to estimate: $[E_T]$, $k_{cat}$ and $K_M$.\n",
    "\n",
    "#### Logistic model for coral reef recovery\n",
    "\n",
    "$\\dfrac{\\mathrm{d}C(t)}{\\mathrm{d}t} = rC(t) \\left(1 - \\dfrac{C(t)}{K} \\right), \\,\\, C(0) = C_0$, where $t$ is time since coral reef recovery began (model input), $C(t)$ is coral reef cover at time $t$ (model output), and there are 3 parameters to estimate: $C_0$, $r$ and $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 889
    },
    "id": "deUfsNepdoLC",
    "outputId": "08d8a537-d2b0-432d-adaf-7b498f5eb0be"
   },
   "outputs": [],
   "source": [
    "## Toy exponential model\n",
    "def y_model_function(x, a, b):\n",
    "   y_model = a*np.exp(b*x)\n",
    "   return y_model\n",
    "\n",
    "# ## Michaelis-Menten kinetics model\n",
    "# def y_model_function(S, E_T, k_cat, K_M):\n",
    "#    nu = k_cat*E_T*S/(K_M+S)\n",
    "#    return nu\n",
    "\n",
    "# ## Coral reef recovery model\n",
    "# def y_model_function(T, C_0, K, r):\n",
    "#     vector_of_C_at_times_T = np.zeros(len(T))\n",
    "#     for obs_number in range(len(T)):\n",
    "#         # For each observation, re-run the model\n",
    "#         if T[obs_number] == 0:\n",
    "#             vector_of_C_at_times_T[obs_number] = C_0\n",
    "#             # No need to run the model if we are looking at an observation at t = 0!\n",
    "#         else:      \n",
    "#             approx_dt = 1 # (Approximate) timestep of model\n",
    "#             num_t_values = round(T[obs_number]/approx_dt) # Number of timesteps in the model.\n",
    "#             t = np.linspace(0, T[obs_number], num=num_t_values+1) # Set up the vector for time t.\n",
    "#             dt = t[1]-t[0] # \"Delta t\", i.e. how spaced apart each time value t actually is.\n",
    "#             C = np.zeros(num_t_values+1)\n",
    "#             C[0]=C_0\n",
    "#             for n in range(num_t_values): # Running the ODE model!\n",
    "#                 C[n+1]=C[n] + dt * r * C[n] * (1 - C[n]/K)\n",
    "#             vector_of_C_at_times_T[obs_number] = C[-1]\n",
    "#     return vector_of_C_at_times_T\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4  Fit the model to the data!\n",
    "\n",
    "Now that we have defined the model, and the data, we can use in-built packages to estimate the *best-fit* model parameters $\\theta_{\\mathrm{MLE}}$ from the data. Here we will use *lmfit*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_structure = Model(y_model_function)\n",
    "# Build the model (a necesssary code step for \"lmfit\", this code step may be unnecessary in other programs)\n",
    "\n",
    "params = model_structure.make_params(a=1,b=1)\n",
    "# Input some initial guesses for the parameters we are estimating!\n",
    "\n",
    "fitted_model = model_structure.fit(y_obs, params, x=x_obs)\n",
    "print(fitted_model.fit_report())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Plot the model-data fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_obs, y_obs, 'ok')\n",
    "\n",
    "x_model = np.linspace(min(x_obs),max(x_obs),101)\n",
    "y_model = fitted_model.eval(x=x_model)\n",
    "\n",
    "plt.plot(x_model, y_model, 'r')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Visualisation\n",
    "\n",
    "In this module, we are going to focus on visualising our calibration outputs.\n",
    "\n",
    "We have already seen in Section 1.5 how to plot the model-data fit of the MLE.\n",
    "\n",
    "We will do five types of visualisation in this module (2.1-2.5), but only need frequentist model-data calibration outputs for two of them: **2.1** Confidence interval predictions, and **2.4** Extract best-fit parameter estimates (for comparison to Bayesian inference parameter estimates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Confidence interval predictions\n",
    "\n",
    "Luckily for us, the in-built package we have used here for frequentist model-data calibration can do the hard work of calculating one type of confidence interval for us - the confidence interval for the *uncertainty in the best-fit predictions*.\n",
    "\n",
    "If you've run the code in Section 1.5, the code block below is the only new code that is needed.\n",
    "\n",
    "**Note \\#1:** The value assigned to the function argument 'sigma' determines whether the confidence interval is $\\pm 1 \\sigma$, $\\pm 2 \\sigma$ or $\\pm 3 \\sigma$. Simply change this value to 1, 2 or 3 to switch it! (You can also choose any other multiple of sigma if you so fancy.)\n",
    "\n",
    "**Note \\#2:** The code block below is set up for the toy exponential model example: you'll need to change 'x=x_model' to 'S=x_model' or 'T=x_model' to make it work for the Michaelis-Menten kinetics or coral reef recovery models, respectively.\n",
    "\n",
    "**Note \\#3:** You will likely get something looking quite awful for anything more complicated than the toy exponential model example... Check the uncertainty in parameter estimates obtained in Section 1.4 for a hint at why this might be the case!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confidence interval for the uncertainty in the best-fit predictions.\n",
    "# Set sigma = 2 for a 95% confidence interval:\n",
    "y_model_CI = fitted_model.eval_uncertainty(x=x_model, sigma=2)\n",
    "\n",
    "plt.plot(x_obs, y_obs, 'ok')\n",
    "plt.plot(x_model, y_model, 'r')\n",
    "plt.fill_between(x_model,\n",
    "                y_model+y_model_CI,\n",
    "                y_model-y_model_CI,\n",
    "                color='pink',\n",
    "                label='uncertainty band of fit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Extract best-fit parameter estimates (for comparison to Bayesian inference parameter estimates)\n",
    "\n",
    "The below code extracts parameter values (set up here for the toy exponential model example) for comparison to the Bayesian inference parameter estimates - see Section 2.4 of that Python script!\n",
    "\n",
    "We are also here calculating the *root-mean-square error* via\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mbox{RMSE} = \\sqrt{\\dfrac{ \\displaystyle\\sum_{k=1}^{N_{\\mathrm{obs}}} \\left( y_{\\mathrm{obs},k} - y_{\\mathrm{model}} \\left(\\mathbf{x}_{\\mathrm{obs},k}, \\mathbf{\\theta} \\right) \\right)^2 }{N_{\\mathrm{obs}} - N_{\\theta}} } \\end{equation*}\n",
    "\n",
    "as this number is loosely analogous to the standard deviation $\\sigma$ that Bayesian inference will also estimate. (Note that sometimes, when RMSE is calculated, $N_{\\theta}$ is dropped off from this formula.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_MLE = fitted_model.params['a'].value\n",
    "b_MLE = fitted_model.params['b'].value\n",
    "residuals = fitted_model.residual\n",
    "RMSE = math.sqrt(sum(np.square(residuals))/(len(residuals)-2)) # 2 here because N_theta=2 for the toy exponential model example\n",
    "\n",
    "print(f'a_MLE=',a_MLE)\n",
    "print(f'b_MLE=',b_MLE)\n",
    "print(f'RMSE=',RMSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analysis of model slopppiness\n",
    "\n",
    "In this module we are going to focus on the task of calculating the eigenparameters (parameter combinations) of the model, ordered from stiffest (most sensitive to model-data fit) to sloppiest (least sensitive to model-data fit).\n",
    "\n",
    "For frequentist model-data calibrations, we will discuss two sensitivity matrices: the Hessian matrix **H** and the Levenberg-Marquardt Hessian matrix **L**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Hessian matrix\n",
    "\n",
    "Let's start by doing analysis of sloppiness using the *Hessian matrix* **H** as the sensitivity matrix.\n",
    "\n",
    "The code below is set up for the toy exponential model example, but can be changed to other examples by:\n",
    "* Changing the values of theta_MLE, and\n",
    "* Changing the parameter names, and\n",
    "* Changing the number of theta parameters passed to y_model inside the loglikelihood function definition.\n",
    "\n",
    "The code block ends by printing out the eigenparameters, ordered from stiffest to sloppiest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_MLE = np.array([[fitted_model.params['a'].value], \\\n",
    "                       [fitted_model.params['b'].value]])   # theta_MLE for the toy exponential model\n",
    "\n",
    "#theta_MLE = np.array([[fitted_model.params['E_T'].value], \\\n",
    "#                      [fitted_model.params['k_cat'].value], \\\n",
    "#                      [fitted_model.params['K_M'].value]])   # theta_MLE for the Michaelis-Menten kinetics models\n",
    "\n",
    "# theta_MLE = np.array([[fitted_model.params['C_0'].value], \\\n",
    "#                       [fitted_model.params['K'].value], \\\n",
    "#                       [fitted_model.params['r'].value]])     # theta_MLE for the coral reef recovery model\n",
    "\n",
    "Parameter_names = ['a','b']              # Ordered parameter names for the toy exponential model\n",
    "#Parameter_names = ['E_T','k_cat','K_M'] # Ordered parameter names for the Michaelis-Menten kinetics models\n",
    "#Parameter_names = ['C_0','K','r']       # Ordered parameter names for the coral reef recovery model\n",
    "\n",
    "# Define loglikelihood function for the present model.\n",
    "def loglikelihood_function(x_obs, y_obs, theta):\n",
    "  params = theta\n",
    "  sigma = 1 # It doesn't matter too much what the value of sigma is!\n",
    "  y_model = y_model_function(x_obs, theta[0], theta[1])\n",
    "            # You'll need to change this when you change the number of parameters in your model!\n",
    "  N_obs = len(y_obs)\n",
    "  Loglikelihood = -0.5*N_obs*np.log(2*math.pi) \\\n",
    "                  - 0.5*N_obs*np.log( np.square(sigma)) \\\n",
    "                  - 0.5 * np.sum( np.square((y_obs-y_model)/sigma) )\n",
    "  return Loglikelihood\n",
    "\n",
    "# Define common things needed to calculate H\n",
    "logL_MLE = loglikelihood_function(x_obs,y_obs,theta_MLE)\n",
    "delta = 1e-4\n",
    "N_theta = len(theta_MLE)\n",
    "H = np.zeros((N_theta,N_theta))\n",
    "\n",
    "# First, calculate the diagonal elements of Hessian matrix H\n",
    "for i in range(N_theta):\n",
    "    theta_up = np.copy(theta_MLE)\n",
    "    theta_up[i]=theta_MLE[i]*(1 + delta)\n",
    "    logL_up = loglikelihood_function(x_obs,y_obs,theta_up)\n",
    "    theta_down = np.copy(theta_MLE)\n",
    "    theta_down[i]=theta_MLE[i]*(1 - delta)\n",
    "    logL_down = loglikelihood_function(x_obs,y_obs,theta_down)\n",
    "    H[i,i]= -1/np.square(delta) * (logL_up - 2*logL_MLE + logL_down)\n",
    "\n",
    "# Second, calculate the off-diagonal elements of Hessian matrix H\n",
    "for i in range(N_theta):\n",
    "    for j in range(i+1,N_theta):\n",
    "        theta_up_up = np.copy(theta_MLE)\n",
    "        theta_up_up[i]=theta_MLE[i]*(1 + delta/2)\n",
    "        theta_up_up[j]=theta_MLE[j]*(1 + delta/2)\n",
    "        logL_up_up = loglikelihood_function(x_obs,y_obs,theta_up_up)\n",
    "        theta_up_down = np.copy(theta_MLE)\n",
    "        theta_up_down[i]=theta_MLE[i]*(1 + delta/2)\n",
    "        theta_up_down[j]=theta_MLE[j]*(1 - delta/2)\n",
    "        logL_up_down = loglikelihood_function(x_obs,y_obs,theta_up_down)\n",
    "        theta_down_up = np.copy(theta_MLE)\n",
    "        theta_down_up[i]=theta_MLE[i]*(1 - delta/2)\n",
    "        theta_down_up[j]=theta_MLE[j]*(1 + delta/2)\n",
    "        logL_down_up = loglikelihood_function(x_obs,y_obs,theta_down_up)\n",
    "        theta_down_down = np.copy(theta_MLE)\n",
    "        theta_down_down[i]=theta_MLE[i]*(1 - delta/2)\n",
    "        theta_down_down[j]=theta_MLE[j]*(1 - delta/2)\n",
    "        logL_down_down = loglikelihood_function(x_obs,y_obs,theta_down_down)\n",
    "        H[i,j] = -1/np.square(delta) * (logL_up_up - logL_up_down - logL_down_up + logL_down_down)\n",
    "        H[j,i] = np.copy(H[i,j])\n",
    "\n",
    "lamda, v = np.linalg.eig(H)\n",
    "# Perform eigendecomposition on the Hessian matrix H!\n",
    "\n",
    "reordered_lamda_elements = lamda.argsort()[::-1]\n",
    "lamda = lamda[reordered_lamda_elements]\n",
    "v = v[:,reordered_lamda_elements]\n",
    "# Reorder eigenvalues from largest to smallest, and reorder the eigenvectors accordingly\n",
    "\n",
    "lamda = lamda/max(lamda)\n",
    "# Rescale the eigenvalues.\n",
    "\n",
    "for n in range(N_theta):\n",
    "    if abs(min(v[:,n])) > max(v[:,n]):\n",
    "        v[:,n]=v[:,n]/min(v[:,n])\n",
    "    else:\n",
    "        v[:,n]=v[:,n]/max(v[:,n])\n",
    "# Rescale each eigenvector so that the leading parameter within each eigenvector has index of +1\n",
    "\n",
    "for j in range(len(theta_MLE)):\n",
    "    Eigenparameter = Parameter_names[0] + '^' + str(round(v[0,j],2))\n",
    "    for i in range(1,len(theta_MLE)):\n",
    "        Eigenparameter = Eigenparameter + ' x ' + Parameter_names[i] + '^' + str(round(v[i,j],2))\n",
    "    print(f'Eigenparameter',j+1,'is',Eigenparameter,'corresponding to lambda_j/lambda_1=',round(lamda[j],3))\n",
    "# Report the eigenparameters!\n",
    "# Note we don't bother here to remove parameters with indices between -0.2 and +0.2, I leave that you to interpret correctly!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Levenberg-Marquardt Hessian matrix L\n",
    "\n",
    "We can alternatively use the Levenberg-Marquardt Hessian matrix **L** matrix as the sensitivity matrix: we should get the same result, but it may be slightly faster computationally. (However, for the small models we are analysing in this course, the only time you might notice *any* difference in computation time is for the coral model!)\n",
    "\n",
    "Like Section 3.1, the code below is set up for the toy exponential model example, but can be changed to other examples by:\n",
    "* Changing the values of theta_MLE, and\n",
    "* Changing the parameter names, and\n",
    "* Changing the number of theta parameters passed to y_model inside the $r$ function definition.\n",
    "\n",
    "The code block ends by printing out the eigenparameters, ordered from stiffest to sloppiest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_MLE = np.array([[fitted_model.params['a'].value], \\\n",
    "                       [fitted_model.params['b'].value]])   # theta_MLE for the toy exponential model\n",
    "\n",
    "#theta_MLE = np.array([[fitted_model.params['E_T'].value], \\\n",
    "#                      [fitted_model.params['k_cat'].value], \\\n",
    "#                      [fitted_model.params['K_M'].value]])   # theta_MLE for the Michaelis-Menten kinetics models\n",
    "\n",
    "# theta_MLE = np.array([[fitted_model.params['C_0'].value], \\\n",
    "#                       [fitted_model.params['K'].value], \\\n",
    "#                       [fitted_model.params['r'].value]])     # theta_MLE for the coral reef recovery model\n",
    "\n",
    "Parameter_names = ['a','b']              # Ordered parameter names for the toy exponential model\n",
    "#Parameter_names = ['E_T','k_cat','K_M'] # Ordered parameter names for the Michaelis-Menten kinetics models\n",
    "#Parameter_names = ['C_0','K','r']       # Ordered parameter names for the coral reef recovery model\n",
    "\n",
    "def r_function(x_obs, y_obs, k, theta):\n",
    "  sigma = 1 # Again it doesn't matter too much what the value of sigma is!\n",
    "  y_model_k = y_model_function([x_obs[k]], theta[0], theta[1])\n",
    "    # You'll need to change this when you change the number of parameters in your model!\n",
    "  r_k_theta = (y_obs[k]-y_model_k)/sigma\n",
    "  return r_k_theta\n",
    "\n",
    "# Define common things needed to calculate L\n",
    "delta = 1e-4\n",
    "N_obs = len(y_obs)\n",
    "N_theta = len(theta_MLE)\n",
    "dr_dlogtheta = np.zeros((N_obs,N_theta))\n",
    "L = np.zeros((N_theta,N_theta))\n",
    "\n",
    "# First, calculate all required derivatives d(r_k)/d(log theta_i)\n",
    "for i in range(N_theta):\n",
    "    theta_up = np.copy(theta_MLE)\n",
    "    theta_up[i] = theta_MLE[i]*(1 + delta/2)\n",
    "    theta_down = np.copy(theta_MLE)\n",
    "    theta_down[i] = theta_MLE[i]*(1 - delta/2)\n",
    "    for k in range(N_obs):\n",
    "        dr_dlogtheta[k,i] = (r_function(x_obs,y_obs,k,theta_up) \\\n",
    "                             - r_function(x_obs,y_obs,k,theta_down)) / delta\n",
    "\n",
    "# Second, calculate all elements of Levenberg-Marquardt Hessian matrix L\n",
    "for i in range(N_theta):\n",
    "    for j in range(N_theta):\n",
    "        for k in range(N_obs):\n",
    "            L[i,j] = L[i,j] + dr_dlogtheta[k,i]*dr_dlogtheta[k,j]\n",
    "            \n",
    "lamda, v = np.linalg.eig(L)\n",
    "# Perform eigendecomposition on the Levenberg-Marquardt Hessian matrix L!\n",
    "\n",
    "reordered_lamda_elements = lamda.argsort()[::-1]\n",
    "lamda = lamda[reordered_lamda_elements]\n",
    "v = v[:,reordered_lamda_elements]\n",
    "# Reorder eigenvalues from largest to smallest, and reorder the eigenvectors accordingly\n",
    "\n",
    "lamda = lamda/max(lamda)\n",
    "# Rescale the eigenvalues.\n",
    "\n",
    "for n in range(N_theta):\n",
    "    if abs(min(v[:,n])) > max(v[:,n]):\n",
    "        v[:,n]=v[:,n]/min(v[:,n])\n",
    "    else:\n",
    "        v[:,n]=v[:,n]/max(v[:,n])\n",
    "# Rescale each eigenvector so that the leading parameter within each eigenvector has index of +1\n",
    "\n",
    "for j in range(len(theta_MLE)):\n",
    "    Eigenparameter = Parameter_names[0] + '^' + str(round(v[0,j],2))\n",
    "    for i in range(1,len(theta_MLE)):\n",
    "        Eigenparameter = Eigenparameter + ' x ' + Parameter_names[i] + '^' + str(round(v[i,j],2))\n",
    "    print(f'Eigenparameter',j+1,'is',Eigenparameter,'corresponding to lambda_j/lambda_1=',round(lamda[j],3))\n",
    "# Report the eigenparameters!\n",
    "# Note we don't bother here to remove parameters with indices between -0.2 and +0.2, I leave that you to interpret correctly! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
